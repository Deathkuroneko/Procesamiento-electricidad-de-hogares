# Big Data â€“ Arquitectura Kappa
AnÃ¡lisis de consumo energÃ©tico usando Kafka, Spark, HDFS y MongoDB.

Arquitectura:
- Ingesta: Kafka
- Procesamiento: Spark (Streaming + Batch)
- Almacenamiento: HDFS + MongoDB
- VisualizaciÃ³n: Superset

Proyecto acadÃ©mico â€“ Taller I

Inicializar docker:
docker compose down
docker compose up -d

Pruebas:
Loger Master: docker logs spark --tail 30
    Resultadosimilar : Starting Spark master
                        SparkUI available at http://spark:8080


docker logs spark-worker --tail 30: Successfully registered with master spark://spark:7077


http://localhost:8080 - Spark Master
 

 - CREACION DE TOPIC:

 docker exec -it kafka kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic energy-consumption

    verificacion:
        docker exec -it kafka kafka-topics --bootstrap-server localhost:9092 --list

- INSTALAR PYTHON Y PANDAS
    python -m pip install kafka-python pandas
    python -m pip install kafka-python

- Correr Producer.py
    - cd kafka
    - python producer.py

- VER TRASNMISION DE DATOS:
docker exec -it kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic energy-consumption --from-beginning
- EJECUTAR EL JOB EN SPARK
docker exec -it spark spark-submit --master spark://spark-master:7077 /opt/spark/jobs/energy_stream.py


TXT (Kaggle)
   â†“
Preprocesamiento (Local / Colab)
   â†“
CSV limpio
   â†“
Kafka Producer
   â†“
Kafka Topic
   â†“
Spark Streaming
   â†“
HDFS (histÃ³rico)
   â†“
MongoDB (resultados)


ğŸ‘‰ Paso 1 (hecho): TXT â†’ CSV
ğŸ‘‰ Paso 2 (siguiente): Kafka Producer leyendo CSV
ğŸ‘‰ Paso 3: Spark Streaming
ğŸ‘‰ Paso 4: Guardar en HDFS y MongoDB

bigdata-kappa-energy/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ kafka/
â”‚   â””â”€â”€ producer.py
â”‚
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ streaming.py
â”‚   â””â”€â”€ batch.py
â”‚
â”œâ”€â”€ hdfs/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ mongodb/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md

Paso 3
Kafka (Ãºnica fuente)
   â†“
Spark Structured Streaming
   â†“            â†“
 MongoDB        HDFS
 (speed)     (batch/histÃ³rico)

spark/streaming.py

Descargar el JAR:

mongo-spark-connector_2.13-10.2.0.jar


Colocarlo en:

spark/jars/

Y en docker-compose.yml (spark + worker):

environment:
  - SPARK_EXTRA_CLASSPATH=/opt/spark/jars/*

  3ï¸âƒ£ Copiar el job al contenedor Spark

Desde la raÃ­z del proyecto:

docker cp spark/streaming.py spark:/opt/spark/jobs/streaming.py


Verifica:

docker exec -it spark ls /opt/spark/jobs

4ï¸âƒ£ Ejecutar Spark Streaming
docker exec -it spark spark-submit \
--master spark://spark:7077 \
/opt/spark/jobs/streaming.py

5ï¸âƒ£ Prueba REAL (pipeline completo)

1ï¸âƒ£ Arranca Spark Streaming
2ï¸âƒ£ En otra terminal ejecuta el producer.py
3ï¸âƒ£ Observa:

docker logs spark --tail 30


4ï¸âƒ£ Revisa HDFS:

docker exec -it hdfs-namenode hdfs dfs -ls /energy/raw


5ï¸âƒ£ Revisa Mongo:

docker exec -it mongodb mongosh
use energy
db.streaming.find().limit(5)

| Principio Kappa | Evidencia                  |
| --------------- | -------------------------- |
| Fuente Ãºnica    | Kafka                      |
| Streaming       | Spark Structured Streaming |
| HistÃ³rico       | HDFS                       |
| Reprocesar      | Spark lee HDFS             |
| Simplicidad     | Un pipeline                |
